{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\most2\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Importing the necessary modules\n",
    "from absl import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow_docs.vis import embed\n",
    "\n",
    "logging.set_verbosity(logging.ERROR)\n",
    "\n",
    "# Some modules to help with reading the UCF101 dataset.\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import tempfile\n",
    "import ssl\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import imageio\n",
    "from IPython import display\n",
    "\n",
    "from urllib import request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are some constants to set up that will be used in feature extraction and RNN.\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "MAX_SEQ_LENGTH = 20\n",
    "NUM_FEATURES = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Helper functions for the UCF101 dataset\n",
    "\n",
    "# Utilities to fetch videos from UCF101 dataset\n",
    "UCF_ROOT = \"https://www.crcv.ucf.edu/THUMOS14/UCF101/UCF101/\"\n",
    "_VIDEO_LIST = None\n",
    "# Setting the cache directory to store the videos not to keep downloading them everytime\n",
    "_CACHE_DIR = \"C://Users//most2//AppData//Local//Temp//tmprw3n5sec\"\n",
    "# As of July 2020, crcv.ucf.edu doesn't use a certificate accepted by the\n",
    "# default Colab environment anymore.\n",
    "unverified_context = ssl._create_unverified_context()\n",
    "\n",
    "def list_ucf_videos():\n",
    "  \"\"\"Lists videos available in UCF101 dataset.\"\"\"\n",
    "  global _VIDEO_LIST\n",
    "  if not _VIDEO_LIST:\n",
    "    index = request.urlopen(UCF_ROOT, context=unverified_context).read().decode(\"utf-8\")\n",
    "    videos = re.findall(\"(v_[\\w_]+\\.avi)\", index)\n",
    "    _VIDEO_LIST = sorted(set(videos))\n",
    "  return list(_VIDEO_LIST)\n",
    "\n",
    "def fetch_ucf_video(video):\n",
    "  \"\"\"Fetches a video and cache into local filesystem.\"\"\"\n",
    "  cache_path = os.path.join(_CACHE_DIR, video)\n",
    "  if not os.path.exists(cache_path):\n",
    "    urlpath = request.urljoin(UCF_ROOT, video)\n",
    "    print(\"Fetching %s => %s\" % (urlpath, cache_path))\n",
    "    data = request.urlopen(urlpath, context=unverified_context).read()\n",
    "    open(cache_path, \"wb\").write(data)\n",
    "  return cache_path\n",
    "\n",
    "# Utilities to open video files using CV2\n",
    "def crop_center_square(frame):\n",
    "  y, x = frame.shape[0:2]\n",
    "  min_dim = min(y, x)\n",
    "  start_x = (x // 2) - (min_dim // 2)\n",
    "  start_y = (y // 2) - (min_dim // 2)\n",
    "  return frame[start_y:start_y+min_dim,start_x:start_x+min_dim]\n",
    "\n",
    "def load_video(path, max_frames=0, resize=(224, 224)):\n",
    "  cap = cv2.VideoCapture(path)\n",
    "  frames = []\n",
    "  i = 0\n",
    "  try:\n",
    "    while True:\n",
    "      ret, frame = cap.read()\n",
    "      if not ret:\n",
    "        break\n",
    "      if (i%5 == 0):\n",
    "        frame = crop_center_square(frame)\n",
    "        frame = cv2.resize(frame, resize)\n",
    "        frame = frame[:, :, [2, 1, 0]]\n",
    "        frames.append(frame)\n",
    "      i += 1\n",
    "      if len(frames) == max_frames:\n",
    "        break\n",
    "  finally:\n",
    "    cap.release()\n",
    "  return np.array(frames) / 255.0\n",
    "\n",
    "def to_gif(images):\n",
    "  converted_images = np.clip(images * 255, 0, 255).astype(np.uint8)\n",
    "  imageio.mimsave('./animation.gif', converted_images, duration=40)\n",
    "  return embed.embed_file('./animation.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13320 videos in 101 categories.\n",
      "ApplyEyeMakeup        145 videos (v_ApplyEyeMakeup_g01_c01.avi, v_ApplyEyeMakeup_g01_c02.avi, ...)\n",
      "ApplyLipstick         114 videos (v_ApplyLipstick_g01_c01.avi, v_ApplyLipstick_g01_c02.avi, ...)\n",
      "Archery               145 videos (v_Archery_g01_c01.avi, v_Archery_g01_c02.avi, ...)\n",
      "BabyCrawling          132 videos (v_BabyCrawling_g01_c01.avi, v_BabyCrawling_g01_c02.avi, ...)\n",
      "BalanceBeam           108 videos (v_BalanceBeam_g01_c01.avi, v_BalanceBeam_g01_c02.avi, ...)\n",
      "BandMarching          155 videos (v_BandMarching_g01_c01.avi, v_BandMarching_g01_c02.avi, ...)\n",
      "BaseballPitch         150 videos (v_BaseballPitch_g01_c01.avi, v_BaseballPitch_g01_c02.avi, ...)\n",
      "BasketballDunk        131 videos (v_BasketballDunk_g01_c01.avi, v_BasketballDunk_g01_c02.avi, ...)\n",
      "Basketball            134 videos (v_Basketball_g01_c01.avi, v_Basketball_g01_c02.avi, ...)\n",
      "BenchPress            160 videos (v_BenchPress_g01_c01.avi, v_BenchPress_g01_c02.avi, ...)\n",
      "Biking                134 videos (v_Biking_g01_c01.avi, v_Biking_g01_c02.avi, ...)\n",
      "Billiards             150 videos (v_Billiards_g01_c01.avi, v_Billiards_g01_c02.avi, ...)\n",
      "BlowDryHair           131 videos (v_BlowDryHair_g01_c01.avi, v_BlowDryHair_g01_c02.avi, ...)\n",
      "BlowingCandles        109 videos (v_BlowingCandles_g01_c01.avi, v_BlowingCandles_g01_c02.avi, ...)\n",
      "BodyWeightSquats      112 videos (v_BodyWeightSquats_g01_c01.avi, v_BodyWeightSquats_g01_c02.avi, ...)\n",
      "Bowling               155 videos (v_Bowling_g01_c01.avi, v_Bowling_g01_c02.avi, ...)\n",
      "BoxingPunchingBag     163 videos (v_BoxingPunchingBag_g01_c01.avi, v_BoxingPunchingBag_g01_c02.avi, ...)\n",
      "BoxingSpeedBag        134 videos (v_BoxingSpeedBag_g01_c01.avi, v_BoxingSpeedBag_g01_c02.avi, ...)\n",
      "BreastStroke          101 videos (v_BreastStroke_g01_c01.avi, v_BreastStroke_g01_c02.avi, ...)\n",
      "BrushingTeeth         131 videos (v_BrushingTeeth_g01_c01.avi, v_BrushingTeeth_g01_c02.avi, ...)\n",
      "CleanAndJerk          112 videos (v_CleanAndJerk_g01_c01.avi, v_CleanAndJerk_g01_c02.avi, ...)\n",
      "CliffDiving           138 videos (v_CliffDiving_g01_c01.avi, v_CliffDiving_g01_c02.avi, ...)\n",
      "CricketBowling        139 videos (v_CricketBowling_g01_c01.avi, v_CricketBowling_g01_c02.avi, ...)\n",
      "CricketShot           167 videos (v_CricketShot_g01_c01.avi, v_CricketShot_g01_c02.avi, ...)\n",
      "CuttingInKitchen      110 videos (v_CuttingInKitchen_g01_c01.avi, v_CuttingInKitchen_g01_c02.avi, ...)\n",
      "Diving                150 videos (v_Diving_g01_c01.avi, v_Diving_g01_c02.avi, ...)\n",
      "Drumming              161 videos (v_Drumming_g01_c01.avi, v_Drumming_g01_c02.avi, ...)\n",
      "Fencing               111 videos (v_Fencing_g01_c01.avi, v_Fencing_g01_c02.avi, ...)\n",
      "FieldHockeyPenalty    126 videos (v_FieldHockeyPenalty_g01_c01.avi, v_FieldHockeyPenalty_g01_c02.avi, ...)\n",
      "FloorGymnastics       125 videos (v_FloorGymnastics_g01_c01.avi, v_FloorGymnastics_g01_c02.avi, ...)\n",
      "FrisbeeCatch          126 videos (v_FrisbeeCatch_g01_c01.avi, v_FrisbeeCatch_g01_c02.avi, ...)\n",
      "FrontCrawl            137 videos (v_FrontCrawl_g01_c01.avi, v_FrontCrawl_g01_c02.avi, ...)\n",
      "GolfSwing             139 videos (v_GolfSwing_g01_c01.avi, v_GolfSwing_g01_c02.avi, ...)\n",
      "Haircut               130 videos (v_Haircut_g01_c01.avi, v_Haircut_g01_c02.avi, ...)\n",
      "HammerThrow           150 videos (v_HammerThrow_g01_c01.avi, v_HammerThrow_g01_c02.avi, ...)\n",
      "Hammering             140 videos (v_Hammering_g01_c01.avi, v_Hammering_g01_c02.avi, ...)\n",
      "HandstandPushups      128 videos (v_HandstandPushups_g01_c01.avi, v_HandstandPushups_g01_c02.avi, ...)\n",
      "HandstandWalking      111 videos (v_HandstandWalking_g01_c01.avi, v_HandstandWalking_g01_c02.avi, ...)\n",
      "HeadMassage           147 videos (v_HeadMassage_g01_c01.avi, v_HeadMassage_g01_c02.avi, ...)\n",
      "HighJump              123 videos (v_HighJump_g01_c01.avi, v_HighJump_g01_c02.avi, ...)\n",
      "HorseRace             124 videos (v_HorseRace_g01_c01.avi, v_HorseRace_g01_c02.avi, ...)\n",
      "HorseRiding           164 videos (v_HorseRiding_g01_c01.avi, v_HorseRiding_g01_c02.avi, ...)\n",
      "HulaHoop              125 videos (v_HulaHoop_g01_c01.avi, v_HulaHoop_g01_c02.avi, ...)\n",
      "IceDancing            158 videos (v_IceDancing_g01_c01.avi, v_IceDancing_g01_c02.avi, ...)\n",
      "JavelinThrow          117 videos (v_JavelinThrow_g01_c01.avi, v_JavelinThrow_g01_c02.avi, ...)\n",
      "JugglingBalls         121 videos (v_JugglingBalls_g01_c01.avi, v_JugglingBalls_g01_c02.avi, ...)\n",
      "JumpRope              144 videos (v_JumpRope_g01_c01.avi, v_JumpRope_g01_c02.avi, ...)\n",
      "JumpingJack           123 videos (v_JumpingJack_g01_c01.avi, v_JumpingJack_g01_c02.avi, ...)\n",
      "Kayaking              141 videos (v_Kayaking_g01_c01.avi, v_Kayaking_g01_c02.avi, ...)\n",
      "Knitting              123 videos (v_Knitting_g01_c01.avi, v_Knitting_g01_c02.avi, ...)\n",
      "LongJump              131 videos (v_LongJump_g01_c01.avi, v_LongJump_g01_c02.avi, ...)\n",
      "Lunges                127 videos (v_Lunges_g01_c01.avi, v_Lunges_g01_c02.avi, ...)\n",
      "MilitaryParade        125 videos (v_MilitaryParade_g01_c01.avi, v_MilitaryParade_g01_c02.avi, ...)\n",
      "Mixing                136 videos (v_Mixing_g01_c01.avi, v_Mixing_g01_c02.avi, ...)\n",
      "MoppingFloor          110 videos (v_MoppingFloor_g01_c01.avi, v_MoppingFloor_g01_c02.avi, ...)\n",
      "Nunchucks             132 videos (v_Nunchucks_g01_c01.avi, v_Nunchucks_g01_c02.avi, ...)\n",
      "ParallelBars          114 videos (v_ParallelBars_g01_c01.avi, v_ParallelBars_g01_c02.avi, ...)\n",
      "PizzaTossing          113 videos (v_PizzaTossing_g01_c01.avi, v_PizzaTossing_g01_c02.avi, ...)\n",
      "PlayingCello          164 videos (v_PlayingCello_g01_c01.avi, v_PlayingCello_g01_c02.avi, ...)\n",
      "PlayingDaf            151 videos (v_PlayingDaf_g01_c01.avi, v_PlayingDaf_g01_c02.avi, ...)\n",
      "PlayingDhol           164 videos (v_PlayingDhol_g01_c01.avi, v_PlayingDhol_g01_c02.avi, ...)\n",
      "PlayingFlute          155 videos (v_PlayingFlute_g01_c01.avi, v_PlayingFlute_g01_c02.avi, ...)\n",
      "PlayingGuitar         160 videos (v_PlayingGuitar_g01_c01.avi, v_PlayingGuitar_g01_c02.avi, ...)\n",
      "PlayingPiano          105 videos (v_PlayingPiano_g01_c01.avi, v_PlayingPiano_g01_c02.avi, ...)\n",
      "PlayingSitar          157 videos (v_PlayingSitar_g01_c01.avi, v_PlayingSitar_g01_c02.avi, ...)\n",
      "PlayingTabla          111 videos (v_PlayingTabla_g01_c01.avi, v_PlayingTabla_g01_c02.avi, ...)\n",
      "PlayingViolin         100 videos (v_PlayingViolin_g01_c01.avi, v_PlayingViolin_g01_c02.avi, ...)\n",
      "PoleVault             149 videos (v_PoleVault_g01_c01.avi, v_PoleVault_g01_c02.avi, ...)\n",
      "PommelHorse           123 videos (v_PommelHorse_g01_c01.avi, v_PommelHorse_g01_c02.avi, ...)\n",
      "PullUps               100 videos (v_PullUps_g01_c01.avi, v_PullUps_g01_c02.avi, ...)\n",
      "Punch                 160 videos (v_Punch_g01_c01.avi, v_Punch_g01_c02.avi, ...)\n",
      "PushUps               102 videos (v_PushUps_g01_c01.avi, v_PushUps_g01_c02.avi, ...)\n",
      "Rafting               111 videos (v_Rafting_g01_c01.avi, v_Rafting_g01_c02.avi, ...)\n",
      "RockClimbingIndoor    144 videos (v_RockClimbingIndoor_g01_c01.avi, v_RockClimbingIndoor_g01_c02.avi, ...)\n",
      "RopeClimbing          119 videos (v_RopeClimbing_g01_c01.avi, v_RopeClimbing_g01_c02.avi, ...)\n",
      "Rowing                137 videos (v_Rowing_g01_c01.avi, v_Rowing_g01_c02.avi, ...)\n",
      "SalsaSpin             133 videos (v_SalsaSpin_g01_c01.avi, v_SalsaSpin_g01_c02.avi, ...)\n",
      "ShavingBeard          161 videos (v_ShavingBeard_g01_c01.avi, v_ShavingBeard_g01_c02.avi, ...)\n",
      "Shotput               144 videos (v_Shotput_g01_c01.avi, v_Shotput_g01_c02.avi, ...)\n",
      "SkateBoarding         120 videos (v_SkateBoarding_g01_c01.avi, v_SkateBoarding_g01_c02.avi, ...)\n",
      "Skiing                135 videos (v_Skiing_g01_c01.avi, v_Skiing_g01_c02.avi, ...)\n",
      "Skijet                100 videos (v_Skijet_g01_c01.avi, v_Skijet_g01_c02.avi, ...)\n",
      "SkyDiving             110 videos (v_SkyDiving_g01_c01.avi, v_SkyDiving_g01_c02.avi, ...)\n",
      "SoccerJuggling        147 videos (v_SoccerJuggling_g01_c01.avi, v_SoccerJuggling_g01_c02.avi, ...)\n",
      "SoccerPenalty         137 videos (v_SoccerPenalty_g01_c01.avi, v_SoccerPenalty_g01_c02.avi, ...)\n",
      "StillRings            112 videos (v_StillRings_g01_c01.avi, v_StillRings_g01_c02.avi, ...)\n",
      "SumoWrestling         116 videos (v_SumoWrestling_g01_c01.avi, v_SumoWrestling_g01_c02.avi, ...)\n",
      "Surfing               126 videos (v_Surfing_g01_c01.avi, v_Surfing_g01_c02.avi, ...)\n",
      "Swing                 131 videos (v_Swing_g01_c01.avi, v_Swing_g01_c02.avi, ...)\n",
      "TableTennisShot       140 videos (v_TableTennisShot_g01_c01.avi, v_TableTennisShot_g01_c02.avi, ...)\n",
      "TaiChi                100 videos (v_TaiChi_g01_c01.avi, v_TaiChi_g01_c02.avi, ...)\n",
      "TennisSwing           166 videos (v_TennisSwing_g01_c01.avi, v_TennisSwing_g01_c02.avi, ...)\n",
      "ThrowDiscus           130 videos (v_ThrowDiscus_g01_c01.avi, v_ThrowDiscus_g01_c02.avi, ...)\n",
      "TrampolineJumping     119 videos (v_TrampolineJumping_g01_c01.avi, v_TrampolineJumping_g01_c02.avi, ...)\n",
      "Typing                136 videos (v_Typing_g01_c01.avi, v_Typing_g01_c02.avi, ...)\n",
      "UnevenBars            104 videos (v_UnevenBars_g01_c01.avi, v_UnevenBars_g01_c02.avi, ...)\n",
      "VolleyballSpiking     116 videos (v_VolleyballSpiking_g01_c01.avi, v_VolleyballSpiking_g01_c02.avi, ...)\n",
      "WalkingWithDog        123 videos (v_WalkingWithDog_g01_c01.avi, v_WalkingWithDog_g01_c02.avi, ...)\n",
      "WallPushups           130 videos (v_WallPushups_g01_c01.avi, v_WallPushups_g01_c02.avi, ...)\n",
      "WritingOnBoard        152 videos (v_WritingOnBoard_g01_c01.avi, v_WritingOnBoard_g01_c02.avi, ...)\n",
      "YoYo                  128 videos (v_YoYo_g01_c01.avi, v_YoYo_g01_c02.avi, ...)\n"
     ]
    }
   ],
   "source": [
    "# Get the list of videos in the dataset.\n",
    "ucf_videos = list_ucf_videos()\n",
    "  \n",
    "categories = {}\n",
    "for video in ucf_videos:\n",
    "  category = video[2:-12]\n",
    "  if category not in categories:\n",
    "    categories[category] = []\n",
    "  categories[category].append(video)\n",
    "print(\"Found %d videos in %d categories.\" % (len(ucf_videos), len(categories)))\n",
    "\n",
    "for category, sequences in categories.items():\n",
    "  summary = \", \".join(sequences[:2])\n",
    "  print(\"%-20s %4d videos (%s, ...)\" % (category, len(sequences), summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Category                     VideoPath\n",
      "0    ApplyEyeMakeup  v_ApplyEyeMakeup_g01_c01.avi\n",
      "1    ApplyEyeMakeup  v_ApplyEyeMakeup_g01_c02.avi\n",
      "2    ApplyEyeMakeup  v_ApplyEyeMakeup_g01_c03.avi\n",
      "3    ApplyEyeMakeup  v_ApplyEyeMakeup_g01_c04.avi\n",
      "4    ApplyEyeMakeup  v_ApplyEyeMakeup_g01_c05.avi\n",
      "..              ...                           ...\n",
      "395     CliffDiving     v_CliffDiving_g18_c03.avi\n",
      "396     CliffDiving     v_CliffDiving_g18_c04.avi\n",
      "397     CliffDiving     v_CliffDiving_g18_c05.avi\n",
      "398     CliffDiving     v_CliffDiving_g18_c06.avi\n",
      "399     CliffDiving     v_CliffDiving_g18_c07.avi\n",
      "\n",
      "[400 rows x 2 columns]\n",
      "Category\n",
      "ApplyEyeMakeup    100\n",
      "Archery           100\n",
      "Bowling           100\n",
      "CliffDiving       100\n",
      "Name: count, dtype: int64\n",
      "Videos Shape:  (33, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "data = [(category, item) for category, items in categories.items() for item in items]\n",
    "df = pd.DataFrame(data, columns=['Category', 'VideoPath'])\n",
    "\n",
    "# Taking only 100 videos from ApplyEyeMakeup, 100 from CliffDiving, 100 from Archery and 100 from Bowling\n",
    "df = df[(df['Category'] == 'Bowling') | (df['Category'] == 'ApplyEyeMakeup') | (df['Category'] == 'CliffDiving') | (df['Category'] == 'Archery')].groupby('Category').head(100).reset_index(drop=True)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "from IPython.display import Video\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "\n",
    "import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Define helper method to extract frames from videos, the method should take video path as input and return an array of video frames as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFrames(path):\n",
    "    video = load_video(fetch_ucf_video(path), max_frames=20)\n",
    "    return video\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Apply preprocessing steps on each frame ex: (resize to a fixed size to normalize frame dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already done in the load_video function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Split the dataset into training, validation, and testing sets (optional, can be done during data loading)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into train and test\n",
    "\n",
    "train_df = df.sample(frac=0.7)\n",
    "test_df = df.drop(train_df.index).reset_index(drop=True)\n",
    "\n",
    "print(df['Category'].value_counts())\n",
    "print(\"Videos Shape: \", load_video(fetch_ucf_video(df['VideoPath'][0])).shape)\n",
    "\n",
    "# The Validation set is created from the training set during the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Feature Extraction with CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Implement a feature extractor that could extract features form Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.applications import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_extractor():\n",
    "    feature_extractor = keras.applications.InceptionV3(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    )\n",
    "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "\n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "\n",
    "# Extracting features from the videos using this feature extractor model\n",
    "feature_extractor = build_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ApplyEyeMakeup', 'Archery', 'Bowling', 'CliffDiving']\n"
     ]
    }
   ],
   "source": [
    "# Label processor for the categories in the dataset to be numeric just like label encoding (to be used in the RNN)\n",
    "label_processor = keras.layers.StringLookup(\n",
    "    num_oov_indices=0, vocabulary=np.unique(train_df[\"Category\"])\n",
    ")\n",
    "print(label_processor.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame features in train set: (280, 20, 2048)\n",
      "Frame masks in train set: (280, 20)\n"
     ]
    }
   ],
   "source": [
    "def prepare_all_videos(df, root_dir):\n",
    "    num_samples = len(df)\n",
    "    video_paths = df[\"VideoPath\"].values.tolist()\n",
    "    labels = df[\"Category\"].values\n",
    "    labels = keras.ops.convert_to_numpy(label_processor(labels[..., None]))\n",
    "\n",
    "    # 'frame_masks' and 'frame_features' are what we will feed to our sequence model.\n",
    "    # 'frame_masks' will contain a bunch of booleans denoting if a timestep is\n",
    "    # masked with padding or not.\n",
    "    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n",
    "    frame_features = np.zeros(\n",
    "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "    )\n",
    "\n",
    "    # For each video.\n",
    "    for idx, path in enumerate(video_paths):\n",
    "        # Gather all its frames and add a batch dimension.\n",
    "        frames = extractFrames(path)\n",
    "        frames = frames[None, ...]\n",
    "\n",
    "        # Initialize placeholders to store the masks and features of the current video.\n",
    "        temp_frame_mask = np.zeros(\n",
    "            shape=(\n",
    "                1,\n",
    "                MAX_SEQ_LENGTH,\n",
    "            ),\n",
    "            dtype=\"bool\",\n",
    "        )\n",
    "        temp_frame_features = np.zeros(\n",
    "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "        )\n",
    "\n",
    "        # A video has been reduced to 'MAX_SEQ_LENGTH' features.\n",
    "        # Extract features from the frames of the current video.\n",
    "        for i, batch in enumerate(frames):\n",
    "            # The batch is a frame in the video.\n",
    "            video_length = batch.shape[0]\n",
    "            length = min(MAX_SEQ_LENGTH, video_length)\n",
    "            for j in range(length):\n",
    "                temp_frame_features[i, j, :] = feature_extractor.predict(\n",
    "                    batch[None, j, :], verbose=0,\n",
    "                )\n",
    "            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "\n",
    "        frame_features[idx,] = temp_frame_features.squeeze()\n",
    "        frame_masks[idx,] = temp_frame_mask.squeeze()\n",
    "\n",
    "    return (frame_features, frame_masks), labels\n",
    "\n",
    "# Getting the features for both training and testing sets and their corresponding labels\n",
    "train_data, train_labels = prepare_all_videos(train_df, \"train\")\n",
    "test_data, test_labels = prepare_all_videos(test_df, \"test\")\n",
    "\n",
    "print(f\"Frame features in train set: {train_data[0].shape}\")\n",
    "print(f\"Frame masks in train set: {train_data[1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the features locally not to run the CNN model again\n",
    "\n",
    "with open('all_videos_dataset.pkl', 'wb') as f:\n",
    "    pickle.dump([train_data, train_labels, test_data, test_labels], f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame features in train set: (280, 20, 2048)\n",
      "Frame masks in train set: (280, 20)\n"
     ]
    }
   ],
   "source": [
    "loadedFeatures = None\n",
    "\n",
    "# loading the features\n",
    "with open('all_videos_dataset.pkl', 'rb') as f:\n",
    "    train_data, train_labels, test_data, test_labels = pickle.load(f)\n",
    "\n",
    "print(f\"Frame features in train set: {train_data[0].shape}\")\n",
    "print(f\"Frame masks in train set: {train_data[1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Sequence Modeling with RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_31\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_31\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_32      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_33      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">266,368</span> │ input_layer_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ input_layer_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │ lstm_30[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">132</span> │ lstm_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_32      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m2048\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_33      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_30 (\u001b[38;5;33mLSTM\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │    \u001b[38;5;34m266,368\u001b[0m │ input_layer_32[\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ input_layer_33[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_31 (\u001b[38;5;33mLSTM\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │      \u001b[38;5;34m8,320\u001b[0m │ lstm_30[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         │        \u001b[38;5;34m132\u001b[0m │ lstm_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">274,820</span> (1.05 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m274,820\u001b[0m (1.05 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">274,820</span> (1.05 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m274,820\u001b[0m (1.05 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Utility for our sequence model.\n",
    "def get_sequence_model():\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
    "    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "    # We made a mask input because we don't want to condition on padded values (If any).\n",
    "    \n",
    "    # Our RNN model layers were\n",
    "    # Input -> LSTM -> LSTM -> Dense -> Output\n",
    "    x = keras.layers.LSTM(32, return_sequences=True)(\n",
    "        frame_features_input, mask=mask_input\n",
    "    )\n",
    "    x = keras.layers.LSTM(32)(x)\n",
    "    output = keras.layers.Dense(len(class_vocab), activation=\"softmax\")(x)\n",
    "\n",
    "    rnn_model = keras.Model([frame_features_input, mask_input], output)\n",
    "\n",
    "    rnn_model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    rnn_model.summary()\n",
    "    return rnn_model\n",
    "\n",
    "seq_model = get_sequence_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Model Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Combine Features and RNN: Concatenate or feed the extracted features from the CNN into the first layer of the RNN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m5/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6875 - loss: 0.8200\n",
      "Epoch 1: val_loss improved from inf to 0.64782, saving model to model.weights.h5\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.7026 - loss: 0.7914 - val_accuracy: 0.7500 - val_loss: 0.6478\n",
      "Epoch 2/10\n",
      "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7296 - loss: 0.6501\n",
      "Epoch 2: val_loss did not improve from 0.64782\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7270 - loss: 0.6527 - val_accuracy: 0.6905 - val_loss: 0.8051\n",
      "Epoch 3/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6953 - loss: 0.7457 \n",
      "Epoch 3: val_loss did not improve from 0.64782\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6958 - loss: 0.7467 - val_accuracy: 0.7143 - val_loss: 0.7559\n",
      "Epoch 4/10\n",
      "\u001b[1m5/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7300 - loss: 0.7070\n",
      "Epoch 4: val_loss did not improve from 0.64782\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7388 - loss: 0.6933 - val_accuracy: 0.6786 - val_loss: 0.7527\n",
      "Epoch 5/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7797 - loss: 0.6141\n",
      "Epoch 5: val_loss did not improve from 0.64782\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7817 - loss: 0.6115 - val_accuracy: 0.7381 - val_loss: 0.7283\n",
      "Epoch 6/10\n",
      "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7790 - loss: 0.5379\n",
      "Epoch 6: val_loss improved from 0.64782 to 0.56289, saving model to model.weights.h5\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7845 - loss: 0.5348 - val_accuracy: 0.8214 - val_loss: 0.5629\n",
      "Epoch 7/10\n",
      "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8203 - loss: 0.4513\n",
      "Epoch 7: val_loss did not improve from 0.56289\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8155 - loss: 0.4639 - val_accuracy: 0.7619 - val_loss: 0.7624\n",
      "Epoch 8/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7972 - loss: 0.5733 \n",
      "Epoch 8: val_loss did not improve from 0.56289\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7964 - loss: 0.5748 - val_accuracy: 0.7976 - val_loss: 0.6522\n",
      "Epoch 9/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9155 - loss: 0.3922 \n",
      "Epoch 9: val_loss did not improve from 0.56289\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9139 - loss: 0.3921 - val_accuracy: 0.8214 - val_loss: 0.6133\n",
      "Epoch 10/10\n",
      "\u001b[1m5/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8770 - loss: 0.3660\n",
      "Epoch 10: val_loss did not improve from 0.56289\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.8711 - loss: 0.3775 - val_accuracy: 0.6548 - val_loss: 1.1061\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8073 - loss: 0.6077 \n",
      "Test accuracy: 79.17%\n"
     ]
    }
   ],
   "source": [
    "def run_experiment():\n",
    "    # Training the model on the training data and validating on the validation set and saving the best model with the best weights\n",
    "    filepath = \"model.weights.h5\"\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
    "    )\n",
    "\n",
    "    history = seq_model.fit(\n",
    "        [train_data[0], train_data[1]],\n",
    "        train_labels,\n",
    "        validation_split=0.3,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[checkpoint],\n",
    "    )\n",
    "\n",
    "    seq_model.load_weights(filepath)\n",
    "\n",
    "    return history, seq_model\n",
    "\n",
    "\n",
    "_, sequence_model = run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Evaluate the model’s performance on the testing set using metrics like accuracy, precision, recall, and F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "[[0.6427876  0.03286105 0.21471083 0.10964059]\n",
      " [0.40319863 0.03571411 0.21990497 0.34118226]\n",
      " [0.4524123  0.16228    0.02898185 0.3563259 ]\n",
      " [0.15450868 0.41500416 0.02099449 0.4094926 ]\n",
      " [0.6587893  0.01383094 0.29921398 0.02816585]\n",
      " [0.79062736 0.01050435 0.1692647  0.02960349]\n",
      " [0.9258201  0.00897757 0.04457782 0.02062441]\n",
      " [0.87713164 0.0076946  0.08350866 0.03166518]\n",
      " [0.9274963  0.00932378 0.0386866  0.02449337]\n",
      " [0.93470687 0.00741483 0.03586477 0.02201357]\n",
      " [0.93393886 0.01323683 0.02606211 0.02676221]\n",
      " [0.4818963  0.0740191  0.1228486  0.32123604]\n",
      " [0.38306051 0.05443369 0.17626247 0.38624334]\n",
      " [0.20569709 0.0748185  0.6163762  0.1031082 ]\n",
      " [0.3714479  0.18807846 0.13766693 0.30280682]\n",
      " [0.8910512  0.00780288 0.07191209 0.02923385]\n",
      " [0.75600284 0.01439272 0.15535691 0.07424758]\n",
      " [0.9109697  0.00823644 0.05042314 0.03037071]\n",
      " [0.8326947  0.0095828  0.11590599 0.04181653]\n",
      " [0.9053996  0.00789706 0.05640641 0.03029691]\n",
      " [0.80452937 0.03193767 0.09060632 0.07292667]\n",
      " [0.8014761  0.03163525 0.11434069 0.05254797]\n",
      " [0.85963714 0.01935306 0.08067737 0.04033243]\n",
      " [0.92669773 0.00774675 0.04153904 0.02401639]\n",
      " [0.5989836  0.0373627  0.0798201  0.2838336 ]\n",
      " [0.8903724  0.01154718 0.05134309 0.04673732]\n",
      " [0.89233106 0.00831327 0.06373736 0.0356183 ]\n",
      " [0.8875777  0.03448875 0.04527261 0.03266097]\n",
      " [0.6763215  0.02373135 0.12144462 0.17850246]\n",
      " [0.5531438  0.0463396  0.06847692 0.3320397 ]\n",
      " [0.8173609  0.02911187 0.07771526 0.07581199]\n",
      " [0.8592599  0.01163998 0.07408196 0.05501818]\n",
      " [0.2867174  0.08386111 0.01361133 0.6158101 ]\n",
      " [0.02778635 0.8437137  0.0272184  0.10128154]\n",
      " [0.20489131 0.36832824 0.10811893 0.31866154]\n",
      " [0.01801382 0.9067688  0.03403526 0.04118211]\n",
      " [0.02191346 0.895922   0.04000361 0.04216096]\n",
      " [0.4135588  0.02446663 0.45273656 0.10923807]\n",
      " [0.04227028 0.50809014 0.38816223 0.06147728]\n",
      " [0.01432468 0.91755843 0.0199152  0.04820173]\n",
      " [0.22088297 0.18854432 0.3546194  0.23595333]\n",
      " [0.2916671  0.14004155 0.33838496 0.22990634]\n",
      " [0.06274223 0.74856764 0.05615125 0.13253886]\n",
      " [0.23691885 0.05751415 0.02813843 0.6774286 ]\n",
      " [0.17113967 0.06537329 0.01378197 0.7497051 ]\n",
      " [0.01440745 0.90679127 0.02962529 0.04917604]\n",
      " [0.05557153 0.75088364 0.08379193 0.10975289]\n",
      " [0.20315596 0.10071182 0.46987954 0.22625266]\n",
      " [0.01611722 0.90292186 0.02540743 0.0555535 ]\n",
      " [0.03577073 0.8058298  0.05899132 0.09940805]\n",
      " [0.10019958 0.6256361  0.10858148 0.1655828 ]\n",
      " [0.12763289 0.5913974  0.12244555 0.1585242 ]\n",
      " [0.19238771 0.12722942 0.58938164 0.09100126]\n",
      " [0.04175872 0.71444273 0.01496761 0.22883095]\n",
      " [0.41671038 0.09009279 0.08240662 0.4107903 ]\n",
      " [0.47245693 0.03065782 0.264347   0.23253828]\n",
      " [0.0303675  0.01193751 0.94442326 0.0132717 ]\n",
      " [0.02974671 0.01216815 0.94481266 0.01327244]\n",
      " [0.55491227 0.02345764 0.24425775 0.1773723 ]\n",
      " [0.512604   0.02077768 0.36767778 0.09894055]\n",
      " [0.33459592 0.02827751 0.48343122 0.15369537]\n",
      " [0.06456774 0.08176016 0.819847   0.03382506]\n",
      " [0.12911353 0.0291249  0.77234924 0.06941237]\n",
      " [0.4060998  0.03504997 0.41515994 0.14369029]\n",
      " [0.7107481  0.01531965 0.2334061  0.04052615]\n",
      " [0.45930436 0.05914092 0.21443132 0.26712343]\n",
      " [0.20603976 0.13762467 0.36537972 0.29095584]\n",
      " [0.23954761 0.06255869 0.4966711  0.20122263]\n",
      " [0.17728746 0.09144334 0.5901607  0.1411085 ]\n",
      " [0.0321229  0.01283498 0.93688154 0.01816055]\n",
      " [0.03416687 0.01559681 0.9162529  0.03398346]\n",
      " [0.19663492 0.34344277 0.229467   0.23045532]\n",
      " [0.25914136 0.03227611 0.4950418  0.21354076]\n",
      " [0.06911559 0.01375402 0.8958941  0.02123628]\n",
      " [0.76066864 0.01184735 0.18910825 0.03837586]\n",
      " [0.07076863 0.01498841 0.8848049  0.02943804]\n",
      " [0.03506027 0.01960144 0.90029263 0.04504567]\n",
      " [0.07411854 0.0238105  0.7810532  0.12101782]\n",
      " [0.12065104 0.02241384 0.7784827  0.0784524 ]\n",
      " [0.06689174 0.51629955 0.25724486 0.15956393]\n",
      " [0.15842617 0.04735477 0.658737   0.1354821 ]\n",
      " [0.04023619 0.0125985  0.9327431  0.01442224]\n",
      " [0.13479927 0.01614045 0.8263894  0.02267093]\n",
      " [0.0647163  0.01959949 0.88811326 0.02757099]\n",
      " [0.11453518 0.03546274 0.76511765 0.08488451]\n",
      " [0.07645823 0.02536866 0.87555146 0.02262161]\n",
      " [0.12413946 0.02990649 0.80097836 0.04497565]\n",
      " [0.04163802 0.01810654 0.8945024  0.04575299]\n",
      " [0.11017486 0.0246521  0.7249866  0.14018635]\n",
      " [0.08150365 0.04146025 0.0150563  0.8619799 ]\n",
      " [0.11792109 0.04312212 0.01788646 0.8210703 ]\n",
      " [0.09685927 0.04528139 0.01227886 0.8455805 ]\n",
      " [0.0637461  0.75527835 0.02289642 0.15807909]\n",
      " [0.3377134  0.1519354  0.1568721  0.35347918]\n",
      " [0.13574992 0.04461699 0.01522369 0.8044093 ]\n",
      " [0.18871216 0.05280362 0.01431855 0.74416566]\n",
      " [0.2062362  0.05037053 0.01973138 0.72366184]\n",
      " [0.13358305 0.03846258 0.02367679 0.8042776 ]\n",
      " [0.13900432 0.13445328 0.01103271 0.7155097 ]\n",
      " [0.11162005 0.04072158 0.01851804 0.8291403 ]\n",
      " [0.1523197  0.034495   0.4072187  0.4059666 ]\n",
      " [0.08234641 0.05124041 0.00954052 0.8568727 ]\n",
      " [0.140806   0.03845121 0.02028825 0.8004545 ]\n",
      " [0.1628675  0.4082912  0.06473355 0.3641078 ]\n",
      " [0.19710307 0.04217682 0.02099667 0.7397235 ]\n",
      " [0.18295446 0.03764734 0.14041062 0.63898754]\n",
      " [0.18257083 0.03480691 0.35094148 0.4316807 ]\n",
      " [0.2294189  0.04277898 0.02034457 0.7074576 ]\n",
      " [0.32087624 0.0440174  0.06051046 0.5745959 ]\n",
      " [0.14833722 0.03829076 0.05313227 0.7602398 ]\n",
      " [0.15244265 0.0377667  0.22225519 0.58753544]\n",
      " [0.10023196 0.05128506 0.01056381 0.8379192 ]\n",
      " [0.10370575 0.04520441 0.01223926 0.83885056]\n",
      " [0.14017357 0.20696561 0.01215376 0.640707  ]\n",
      " [0.5478928  0.05493507 0.11215114 0.28502098]\n",
      " [0.08496717 0.04337227 0.01351989 0.8581407 ]\n",
      " [0.06573442 0.03912725 0.6144668  0.2806716 ]\n",
      " [0.09048298 0.04342663 0.01501853 0.85107183]\n",
      " [0.09964877 0.0503489  0.01193213 0.83807015]\n",
      " [0.34724763 0.04849922 0.02483402 0.57941914]]\n",
      "[0 0 0 1 0 0 0 0 0 0 0 0 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1 1 1 1\n",
      " 2 1 1 2 2 1 3 3 1 1 2 1 1 1 1 2 1 0 0 2 2 0 0 2 2 2 2 0 0 2 2 2 2 2 1 2 2\n",
      " 0 2 2 2 2 1 2 2 2 2 2 2 2 2 2 3 3 3 1 3 3 3 3 3 3 3 2 3 3 1 3 3 3 3 3 3 3\n",
      " 3 3 3 0 3 2 3 3 3]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3]\n",
      "Training Accuracy:  0.7916666666666666\n"
     ]
    }
   ],
   "source": [
    "# Testing the model on the test data\n",
    "y_pred = sequence_model.predict([test_data[0], test_data[1]])\n",
    "\n",
    "print(y_pred)\n",
    "# Getting the class with the highest probability\n",
    "class_pred = np.argmax(y_pred, axis=1)\n",
    "print(class_pred)\n",
    "print(test_labels.flatten())\n",
    "\n",
    "# Calculating the accuracy\n",
    "print(\"Training Accuracy: \", np.mean(class_pred == test_labels.flatten()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
